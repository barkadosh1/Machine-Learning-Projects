{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import math\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in 3 csv files into dataframes\n",
    "yelp = pd.read_csv('yelp_labelled.txt', sep=\"\\t\", header = None, names = [\"Sentence\",\"Sentiment\"])\n",
    "amazon = pd.read_csv('amazon_cells_labelled.txt', sep=\"\\t\", header = None, names = [\"Sentence\",\"Sentiment\"])\n",
    "imdb = pd.read_csv('imdb_labelled.txt', sep=\"\\t\", header = None, names = [\"Sentence\",\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yelp counts per value\n",
    "yelp.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#amazon counts per value\n",
    "amazon.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    386\n",
       "0    362\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imdb counts per value\n",
    "imdb.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all values in the dataframes lowercase\n",
    "yelp = yelp.apply(lambda x: x.astype(str).str.lower())\n",
    "amazon = amazon.apply(lambda x: x.astype(str).str.lower())\n",
    "imdb = imdb.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to strip away punctuation in our text\n",
    "def strip_punctuation(string_to_strip):\n",
    "    word_array = string_to_strip.split()\n",
    "    ret_string = \"\"\n",
    "    for word in word_array:\n",
    "        new_word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "        ret_string += new_word\n",
    "        ret_string +=\" \"\n",
    "    return ret_string.rstrip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stop words from our text\n",
    "def remove_stops(string_to_remove_stops):\n",
    "    tokenized = string_to_remove_stops.split()\n",
    "    ret_string = ''\n",
    "    stops_removed = [word for word in tokenized if word not in stop_words]\n",
    "    \n",
    "    for word in stops_removed:\n",
    "        ret_string += word\n",
    "        ret_string +=\" \"\n",
    "    return ret_string.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to conform all words to their respective stem words\n",
    "def stem_strings(string_to_stem):\n",
    "    word_array = string_to_stem.split()\n",
    "    ret_string = \"\"\n",
    "    for word in word_array:\n",
    "        new_word = PorterStemmer().stem(word)\n",
    "        ret_string += new_word\n",
    "        ret_string +=\" \"\n",
    "    return ret_string.rstrip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bkadosh1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download and assign english stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stripping, stopping, stemming, and concatinating the reviews for yelp with their sentiment labels\n",
    "yelp_stripped = yelp['Sentence'].apply(strip_punctuation,1)\n",
    "yelp_stopped = yelp_stripped.apply(remove_stops,1)\n",
    "yelp_stemmed = yelp_stopped.apply(stem_strings,1)\n",
    "yelp_sentiment = yelp['Sentiment']\n",
    "yelp_transformed = pd.concat([yelp_stemmed,yelp_sentiment], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stripping, stopping, stemming, and concatinating the reviews for imdb with their sentiment labels\n",
    "imdb_stripped = imdb['Sentence'].apply(strip_punctuation,1)\n",
    "imdb_stopped = imdb_stripped.apply(remove_stops,1)\n",
    "imdb_stemmed = imdb_stopped.apply(stem_strings,1)\n",
    "imdb_sentiment = imdb['Sentiment']\n",
    "imdb_transformed = pd.concat([imdb_stemmed,imdb_sentiment], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stripping, stopping, stemming, and concatinating the reviews for amazon with their sentiment labels\n",
    "amazon_stripped = amazon['Sentence'].apply(strip_punctuation,1)\n",
    "amazon_stopped = amazon_stripped.apply(remove_stops,1)\n",
    "amazon_stemmed = amazon_stopped.apply(stem_strings,1)\n",
    "amazon_sentiment = amazon['Sentiment']\n",
    "amazon_transformed = pd.concat([amazon_stemmed,amazon_sentiment], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split amazon into train and test reviews and labels\n",
    "amazon_train, amazon_test, amazon_label_train, amazon_label_test = train_test_split(amazon_transformed['Sentence'], \n",
    "                                                                                    amazon_transformed['Sentiment'], \n",
    "                                                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split yelp into train and test reviews and labels\n",
    "yelp_train, yelp_test, yelp_label_train, yelp_label_test = train_test_split(yelp_transformed['Sentence'], \n",
    "                                                                            yelp_transformed['Sentiment'], \n",
    "                                                                            test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split imdb into train and test reviews and labels\n",
    "imdb_train, imdb_test, imdb_label_train, imdb_label_test = train_test_split(imdb_transformed['Sentence'], \n",
    "                                                                            imdb_transformed['Sentiment'], \n",
    "                                                                            test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset indices on splits to remove errors/confusion \n",
    "imdb_train = imdb_train.reset_index().iloc[:,1]\n",
    "imdb_test = imdb_test.reset_index().iloc[:,1]\n",
    "imdb_label_train = imdb_label_train.reset_index().iloc[:,1]\n",
    "imdb_label_test = imdb_label_test.reset_index().iloc[:,1]\n",
    "\n",
    "yelp_train = yelp_train.reset_index().iloc[:,1]\n",
    "yelp_test = yelp_test.reset_index().iloc[:,1]\n",
    "yelp_label_train = yelp_label_train.reset_index().iloc[:,1]\n",
    "yelp_label_test = yelp_label_test.reset_index().iloc[:,1]\n",
    "\n",
    "amazon_train = amazon_train.reset_index().iloc[:,1]\n",
    "amazon_test = amazon_test.reset_index().iloc[:,1]\n",
    "amazon_label_train = amazon_label_train.reset_index().iloc[:,1]\n",
    "amazon_label_test = amazon_label_test.reset_index().iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all the training reviews from each dataframe, all the testing reviews from each dataframe\n",
    "#combine all the training labels from each dataframe, all the testing labels from each dataframe\n",
    "training_all = pd.concat([imdb_train, yelp_train, amazon_train], ignore_index=True)\n",
    "testing_all = pd.concat([imdb_test, yelp_test, amazon_test], ignore_index=True)\n",
    "training_labels_all = pd.concat([imdb_label_train, yelp_label_train, amazon_label_train], ignore_index=True)\n",
    "testing_labels_all = pd.concat([imdb_label_test, yelp_label_test, amazon_label_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty vocabulary dictionary \n",
    "vocabulary = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the vocabulary dictionary so that each key has value 0 and so that each key is a unique word among all \n",
    "#training reviews\n",
    "for i in range(len(training_all)):\n",
    "    words = training_all[i].split()\n",
    "    for word in words:\n",
    "        vocabulary[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates an easy to work with vector creater for our dictionary\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "vocab = vec.fit_transform(vocabulary).toarray()\n",
    "word_names = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array of zeroes length of vocabulary to start building set of features (empty one will be removed later)\n",
    "reviews = np.zeros((1, len(vocabulary)))\n",
    "\n",
    "#create a copy of the vocabulary as a feature and adjust counts of words for each review \n",
    "for i in range(len(training_all)):\n",
    "    feature = vocabulary.copy()\n",
    "    for word in training_all[i].split():\n",
    "        feature[word] = feature[word] + 1\n",
    "    review = vec.fit_transform(feature).toarray() \n",
    "    reviews = np.concatenate((reviews, review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of feature vectors of reviews, drop first empty row, reset indices, and add column names \n",
    "#additionally, drop the last column as python did not recognize this special character \n",
    "review_text = pd.DataFrame(reviews)\n",
    "review_text = review_text.drop([0])\n",
    "review_text = review_text.reset_index()\n",
    "review_text = review_text.drop('index', axis=1)\n",
    "review_text.columns = word_names\n",
    "review_text = review_text.drop(review_text.columns[-1], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array of zeroes length of vocabulary to start building set of features (empty one will be removed later)\n",
    "reviews2 = np.zeros((1, len(vocabulary)))\n",
    "\n",
    "#create a copy of the vocabulary as a feature and adjust counts of words for each review (for test set)\n",
    "for i in range(len(testing_all)):\n",
    "    feature = vocabulary.copy()\n",
    "    for word in testing_all[i].split():\n",
    "        if word in feature:\n",
    "            feature[word] = feature[word] + 1\n",
    "    review = vec.fit_transform(feature).toarray() \n",
    "    reviews2 = np.concatenate((reviews2, review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of feature vectors of test reviews, drop first empty row, reset indices, and add column names \n",
    "#additionally, drop the last column as python did not recognize this special character \n",
    "review_text_test = pd.DataFrame(reviews2)\n",
    "review_text_test = review_text_test.drop([0])\n",
    "review_text_test = review_text_test.reset_index()\n",
    "review_text_test = review_text_test.drop('index', axis=1)\n",
    "review_text_test.columns = word_names\n",
    "review_text_test = review_text_test.drop(review_text_test.columns[-1], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#print two examples of feature vectors (extends into next cell as well)\n",
    "training_review_list = review_text.values\n",
    "print(training_review_list[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52.0, 0.0, 46.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 3.0, 3.0, 1.0, 0.0, 1.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 21.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 3.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 20.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 4.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 6.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(training_review_list[14].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a copy of our review training text and log normalizing it\n",
    "review_text_copy = review_text.copy()\n",
    "\n",
    "for i in range(review_text_copy.shape[0]):\n",
    "    for j in range(review_text_copy.shape[1]):\n",
    "        review_text_copy.at[i,review_text.columns[j]] = math.log10(review_text_copy.at[i,review_text.columns[j]]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a copy of our review testing text and log normalizing it\n",
    "review_text_test_copy = review_text_test.copy()\n",
    "\n",
    "for i in range(review_text_test_copy.shape[0]):\n",
    "    for j in range(review_text_test_copy.shape[1]):\n",
    "        review_text_test_copy.at[i,review_text_test.columns[j]] = math.log10(review_text_test_copy.at[i,review_text_test.columns[j]]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total number of positive (1) reviews and negative (0) reviews\n",
    "total_positive = training_labels_all.astype(int).sum()\n",
    "total_negative = len(training_labels_all) - total_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the labels to the log-normalized training set and then split it into a dataframe for positive (1) reviews and\n",
    "#another dataframe for negative (0) reviews \n",
    "review_text_full = review_text_copy.copy()\n",
    "review_text_full['labels'] = training_labels_all.astype(int)\n",
    "\n",
    "review_text_0 = review_text_full[review_text_full['labels'] < 1]\n",
    "review_text_1 =  review_text_full[review_text_full['labels'] > 0]\n",
    "\n",
    "review_text_0 = review_text_0.reset_index()\n",
    "review_text_0 = review_text_0.drop('index', axis=1)\n",
    "\n",
    "review_text_1 = review_text_1.reset_index()\n",
    "review_text_1 = review_text_1.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since this is a long block of code, I have added comments in various places throughout this block\n",
    "\n",
    "#convert positive and negative training dataframes and test dataframe into numpy arrays\n",
    "review_text_train_np_0 = np.array(review_text_0)\n",
    "review_text_train_np_1 = np.array(review_text_1)\n",
    "review_text_test_np = np.array(review_text_test_copy)\n",
    "\n",
    "#empty list for predicted labels for naive bayes classifier\n",
    "test_predicted_labels = list()\n",
    "\n",
    "#for each test review, initialize a list for the review feature vector, and two empty lists for probabilities for \n",
    "#relevant feature vectors for negative and positive reviews, respectively \n",
    "for i in range(len(review_text_test_np)):\n",
    "    probs_0 = list()\n",
    "    probs_1 = list()\n",
    "    test = review_text_test_np[i].tolist()\n",
    "\n",
    "    #for each test feature vector, take the values, count how many times that value appears in the train\n",
    "    #set for positive reviews and in the train set for negative reviews, divide those values by number of positive \n",
    "    #and negative reviews, respectively, and append those final probabilities (per feature) to the \n",
    "    #respective probability array\n",
    "    for i in range(len(test)):\n",
    "        test_word = test[i]\n",
    "        count = 0\n",
    "        for row in review_text_train_np_0:\n",
    "            if row[i] == test_word:\n",
    "                count += 1\n",
    "        prob0 = count/total_negative\n",
    "        probs_0.append(prob0)\n",
    "\n",
    "        count = 0\n",
    "        for row in review_text_train_np_1:\n",
    "            if row[i] == test_word:\n",
    "                count += 1\n",
    "        prob1 = count/total_positive\n",
    "        probs_1.append(prob1)\n",
    "    \n",
    "    #multiply each of the probabilities for the relevant negative and positive reviews, respectively. \n",
    "    #then multiply the product of these independent probabilities by the probability of being a negative\n",
    "    #review and of being a positive review, respectively\n",
    "    naive_prob_0 = np.prod(probs_0)\n",
    "    naive_prob_1 = np.prod(probs_1)     \n",
    "    \n",
    "    naive_prob_0 = naive_prob_0 * total_negative / (total_negative + total_positive)\n",
    "    naive_prob_1 = naive_prob_1 * total_positive / (total_negative + total_positive)\n",
    "    \n",
    "    #if the probability for getting a negative label given the words is greater than the probability of \n",
    "    #getting a positive label given the words, then classify the review as a negative label\n",
    "    #if it is the other way around, classify it as a positive value. If it is a tie, we arbitrarily classify it\n",
    "    #as a 0\n",
    "    if naive_prob_0 >= naive_prob_1:\n",
    "        test_predicted_labels.append(0)\n",
    "    else:\n",
    "        test_predicted_labels.append(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7363636363636363"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the accuracy with our predicted labels vs. actual labels\n",
    "labels_actual = np.array(testing_labels_all.astype(int)).tolist()\n",
    "accuracy_score(labels_actual, test_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code for this confusion matrix plotting function was found at the following link\n",
    "#https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.\n",
    "#html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "#however, it was modified quite a lot for visual changes \n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None):\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None: cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=10)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]), horizontalalignment=\"center\", \n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('\\nPredicted label'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create a confusion matrix for our actual test labels and predicted test labels \n",
    "#uses confusion matrix function defined above to create a visual representation of it \n",
    "plot_confusion_matrix(cm = confusion_matrix(labels_actual, test_predicted_labels), target_names = \n",
    "                      [\"Positive (1)\", \"Negative(0)\"], title = \"Review Sentiment Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7930231560005936"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a logistic regression model with l2 penalty, and print out the mean accuracy score using cross validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l2', tol=0.001, C=5.0, solver='lbfgs', multi_class='multinomial')\n",
    "scores = cross_validate(lr, review_text_copy, training_labels_all.astype(int), cv=10,scoring=('accuracy'),return_train_score=True)\n",
    "scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the logistic regression model with our text and label training data \n",
    "lr.fit(review_text_copy, training_labels_all.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary so that each coefficient value has a key that is the word/column it is associated with\n",
    "#sort the dictionary in increasing order\n",
    "coefs = dict()\n",
    "for i in range(len(lr.coef_[0])):\n",
    "    coefs[review_text_copy.columns[i]] = lr.coef_[0][i]\n",
    "sorted_coefs = sorted(coefs.items(), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', -3.578999012751685),\n",
       " ('poor', -3.2642507308388757),\n",
       " ('worst', -3.004636783819291),\n",
       " ('wast', -2.8630096280696975),\n",
       " ('disappoint', -2.5988454220144597),\n",
       " ('fail', -2.4244840301602797),\n",
       " ('unfortun', -2.2949408990923836),\n",
       " ('noth', -2.2945628228616446),\n",
       " ('slow', -2.2938596756977785),\n",
       " ('averag', -2.2674942085793868),\n",
       " ('avoid', -2.214460585212774),\n",
       " ('aw', -2.2118007963345443),\n",
       " ('lack', -2.19169434114884),\n",
       " ('stupid', -2.1678299387620177),\n",
       " ('bland', -2.1618826842066534),\n",
       " ('horribl', -2.1529817270686853),\n",
       " ('didnt', -2.0905607179077688),\n",
       " ('rude', -2.045240566959359),\n",
       " ('terribl', -1.9292815962061691),\n",
       " ('start', -1.9147670948062994)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the first 20 key-value pairs in the dictionary\n",
    "#this represents the 20 words that have the strongest sentiment/indication of a negative review\n",
    "sorted_coefs[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cool', 2.1985618053437737),\n",
       " ('interest', 2.262796841072671),\n",
       " ('wonder', 2.2789336617002123),\n",
       " ('sturdi', 2.293223918072851),\n",
       " ('well', 2.336399260852371),\n",
       " ('happi', 2.3915816647607846),\n",
       " ('enjoy', 2.417364409044665),\n",
       " ('good', 2.4377651933906197),\n",
       " ('perfect', 2.536737486522542),\n",
       " ('best', 2.579682704423841),\n",
       " ('comfort', 2.6421485683206134),\n",
       " ('fantast', 2.646046762845179),\n",
       " ('beauti', 2.6749125556514546),\n",
       " ('awesom', 2.8738587227057617),\n",
       " ('excel', 3.31130626433015),\n",
       " ('amaz', 3.4558254838350924),\n",
       " ('delici', 3.5183454268008028),\n",
       " ('nice', 3.53812564040485),\n",
       " ('love', 3.816390518341043),\n",
       " ('great', 5.240750665294592)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the last 20 key-value pairs in the dictionary\n",
    "#this represents the 20 words that have the strongest sentiment/indication of a positive review\n",
    "last = len(sorted_coefs)\n",
    "sorted_coefs[last - 20:last]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7734507568713854"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a logistic regression model with l1 penalty, and print out the mean accuracy score using cross validation\n",
    "lr2 = LogisticRegression(penalty='l1', tol=0.01, C = 45.0, solver='saga')\n",
    "scores2 = cross_validate(lr2, review_text_copy, training_labels_all.astype(int), cv=10,scoring=('accuracy'),return_train_score=True)\n",
    "scores2['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=45.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='saga', tol=0.01, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the logistic regression model with our text and label training data \n",
    "lr2.fit(review_text_copy, training_labels_all.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary so that each coefficient value has a key that is the word/column it is associated with\n",
    "#sort the dictionary in increasing order\n",
    "coefs2 = dict()\n",
    "for i in range(len(lr2.coef_[0])):\n",
    "    coefs2[review_text_copy.columns[i]] = lr2.coef_[0][i]\n",
    "sorted_coefs2 = sorted(coefs2.items(), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', -4.4111940331587585),\n",
       " ('disappoint', -3.2753269639108873),\n",
       " ('wast', -2.8690037347973227),\n",
       " ('worst', -2.6156568177686097),\n",
       " ('poor', -2.5564632673959675),\n",
       " ('dont', -2.2652821102846654),\n",
       " ('didnt', -2.0427642458292476),\n",
       " ('noth', -1.7626464934145198),\n",
       " ('would', -1.7513092745140926),\n",
       " ('minut', -1.7495934046694424),\n",
       " ('money', -1.7113708536703807),\n",
       " ('terribl', -1.676169109387608),\n",
       " ('aw', -1.5919046570549757),\n",
       " ('could', -1.5508720566486691),\n",
       " ('wait', -1.5125441907501256),\n",
       " ('slow', -1.5096462502963208),\n",
       " ('old', -1.4490135065642737),\n",
       " ('stupid', -1.4105549779622513),\n",
       " ('wasnt', -1.4030533408595431),\n",
       " ('avoid', -1.3958969650762125)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the first 20 key-value pairs in the dictionary\n",
    "#this represents the 20 words that have the strongest sentiment/indication of a negative review\n",
    "sorted_coefs2[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beauti', 1.4407255241477994),\n",
       " ('fantast', 1.470288847215514),\n",
       " ('friendli', 1.5373653043792137),\n",
       " ('enjoy', 1.6517487816001928),\n",
       " ('perfect', 1.7081421180303455),\n",
       " ('happi', 1.7887004150667392),\n",
       " ('film', 1.8639463457219532),\n",
       " ('awesom', 1.8855070870670565),\n",
       " ('price', 1.8953984388541094),\n",
       " ('wonder', 2.0058015194345007),\n",
       " ('comfort', 2.0521717817916056),\n",
       " ('amaz', 2.2685078920296196),\n",
       " ('delici', 2.5577167582349776),\n",
       " ('well', 2.5731049542554705),\n",
       " ('best', 3.130497294521048),\n",
       " ('excel', 3.1903570837336397),\n",
       " ('nice', 3.464797384325938),\n",
       " ('good', 3.8336812829158755),\n",
       " ('love', 4.4582370133357285),\n",
       " ('great', 7.369656738251705)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the last 10 key-value pairs in the dictionary\n",
    "#this represents the 10 words that have the strongest sentiment/indication of a positive review\n",
    "last2 = len(sorted_coefs)\n",
    "sorted_coefs2[last - 20:last]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty ngram vocabulary dictionary\n",
    "vocabulary_ngram = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the ngram vocabulary dictionary so that each key has value 0 and so that each key is a unique bigram \n",
    "#among all training reviews\n",
    "#for one word reviews, we create a bigram where the second word is just a blank string\n",
    "for i in range(len(training_all)):\n",
    "    words = training_all[i].split()\n",
    "    if len(words) == 1:\n",
    "        word = (words[0], \"\") \n",
    "        vocabulary_ngram[word] = 0\n",
    "    else:   \n",
    "        for j in range(len(words)-1):\n",
    "            word = (words[j], words[j+1])\n",
    "            vocabulary_ngram[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates an easy to work with vector creater for our dictionary\n",
    "vec2 = DictVectorizer()\n",
    "vocab2 = vec2.fit_transform(vocabulary_ngram).toarray()\n",
    "ngram_names = vec2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array of zeroes length of vocabulary to start building set of features (empty one will be removed later)\n",
    "reviews = np.zeros((1, len(vocabulary_ngram)))\n",
    "\n",
    "#create a copy of the vocabulary as a feature and adjust counts of bigrams for each review \n",
    "#again, 1 word reviews are accounted for by adding a second empty string to the first word\n",
    "for i in range(len(training_all)):\n",
    "    words = training_all[i].split()\n",
    "    feature = vocabulary_ngram.copy()\n",
    "    if len(words) == 1:\n",
    "        word = (words[0], \"\") \n",
    "        feature[word] = feature[word] + 1\n",
    "    else:\n",
    "        for j in range(len(words)-1):\n",
    "            word = (words[j], words[j+1])\n",
    "            feature[word] = feature[word] + 1\n",
    "    review = vec2.fit_transform(feature).toarray() \n",
    "    reviews = np.concatenate((reviews, review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of feature vectors of train reviews, drop first empty row, reset indices, and add column names \n",
    "#additionally, drop the last 4 columns as python did not recognize this special character \n",
    "review_text_ngram = pd.DataFrame(reviews)\n",
    "review_text_ngram = review_text_ngram.drop([0])\n",
    "review_text_ngram = review_text_ngram.reset_index()\n",
    "review_text_ngram = review_text_ngram.drop('index', axis=1)\n",
    "review_text_ngram.columns = ngram_names\n",
    "review_text_ngram = review_text_ngram.drop(review_text_ngram.columns[-1], axis =1)\n",
    "review_text_ngram = review_text_ngram.drop(review_text_ngram.columns[-1], axis =1)\n",
    "review_text_ngram = review_text_ngram.drop(review_text_ngram.columns[-1], axis =1)\n",
    "review_text_ngram = review_text_ngram.drop(review_text_ngram.columns[-1], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array of zeroes length of vocabulary to start building set of features (empty one will be removed later)\n",
    "reviews2 = np.zeros((1, len(vocabulary_ngram)))\n",
    "\n",
    "#create a copy of the vocabulary as a feature and adjust counts of bigrams for each test review \n",
    "#again, 1 word reviews are accounted for by adding a second empty string to the first word\n",
    "for i in range(len(testing_all)):\n",
    "    words = testing_all[i].split()\n",
    "    feature = vocabulary_ngram.copy()\n",
    "    if len(words) == 1:\n",
    "        word = (words[0], \"\") \n",
    "        if word in feature:\n",
    "            feature[word] = feature[word] + 1\n",
    "    else:\n",
    "        for j in range(len(words)-1):\n",
    "            word = (words[j], words[j+1])\n",
    "            if word in feature:\n",
    "                feature[word] = feature[word] + 1\n",
    "    review = vec2.fit_transform(feature).toarray() \n",
    "    reviews2 = np.concatenate((reviews2, review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of feature vectors of test reviews, drop first empty row, reset indices, and add column names \n",
    "#additionally, drop the last 4 columns as python did not recognize this special character \n",
    "review_text_test_ngram = pd.DataFrame(reviews2)\n",
    "review_text_test_ngram\n",
    "review_text_test_ngram = review_text_test_ngram.drop([0])\n",
    "review_text_test_ngram = review_text_test_ngram.reset_index()\n",
    "review_text_test_ngram = review_text_test_ngram.drop('index', axis=1)\n",
    "review_text_test_ngram.columns = ngram_names\n",
    "review_text_test_ngram = review_text_test_ngram.drop(review_text_test_ngram.columns[-1], axis =1)\n",
    "review_text_test_ngram = review_text_test_ngram.drop(review_text_test_ngram.columns[-1], axis =1)\n",
    "review_text_test_ngram = review_text_test_ngram.drop(review_text_test_ngram.columns[-1], axis =1)\n",
    "review_text_test_ngram = review_text_test_ngram.drop(review_text_test_ngram.columns[-1], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a copy of our ngram review training text and log normalizing it\n",
    "review_ngram_text_copy = review_text_ngram.copy()\n",
    "\n",
    "for i in range(review_ngram_text_copy.shape[0]):\n",
    "    for j in range(review_ngram_text_copy.shape[1]):\n",
    "        review_ngram_text_copy.at[i,review_text_ngram.columns[j]] = math.log10(review_ngram_text_copy.at[i,review_text_ngram.columns[j]]+1)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a copy of our ngram review testing text and log normalizing it\n",
    "review_text_test_ngram_copy = review_text_test_ngram.copy()\n",
    "\n",
    "for i in range(review_text_test_ngram_copy.shape[0]):\n",
    "    for j in range(review_text_test_ngram_copy.shape[1]):\n",
    "        review_text_test_ngram_copy.at[i,review_text_test_ngram.columns[j]] = math.log10(review_text_test_ngram_copy.at[i,review_text_test_ngram.columns[j]]+1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the labels to the log-normalized training set and then split it into a dataframe for positive (1) reviews and\n",
    "#another dataframe for negative (0) reviews -- for ngram\n",
    "review_text_ngram_full = review_ngram_text_copy.copy()\n",
    "review_text_ngram_full['labels'] = training_labels_all.astype(int)\n",
    "\n",
    "review_text_ngram_0 = review_text_ngram_full[review_text_ngram_full['labels'] < 1]\n",
    "review_text_ngram_1 =  review_text_ngram_full[review_text_ngram_full['labels'] > 0]\n",
    "\n",
    "review_text_ngram_0 = review_text_ngram_0.reset_index()\n",
    "review_text_ngram_0 = review_text_ngram_0.drop('index', axis=1)\n",
    "\n",
    "review_text_ngram_1 = review_text_ngram_1.reset_index()\n",
    "review_text_ngram_1 = review_text_ngram_1.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since this is a long block of code, I have added comments in various places throughout this block\n",
    "\n",
    "#convert positive and negative training dataframes and test dataframe into numpy arrays -- for ngram\n",
    "review_text_train_ngram_np_0 = np.array(review_text_ngram_0)\n",
    "review_text_train_ngram_np_1 = np.array(review_text_ngram_1)\n",
    "review_text_test_np_ngram = np.array(review_text_test_ngram_copy)\n",
    "\n",
    "#empty list for ngram predicted labels for naive bayes classifier\n",
    "test_predicted_labels = list()\n",
    "\n",
    "#for each test review, initialize a list for the review feature vector, and two empty lists for probabilities for \n",
    "#relevant feature vectors for negative and positive reviews, respectively \n",
    "for i in range(len(review_text_test_np_ngram)):\n",
    "    probs_0 = list()\n",
    "    probs_1 = list()\n",
    "    test = review_text_test_np_ngram[i].tolist()\n",
    "\n",
    "    #for each test feature vector, take the values, count how many times that value appears in the train\n",
    "    #set for positive reviews and in the train set for negative reviews, divide those values by number of positive \n",
    "    #and negative reviews, respectively, and append those final probabilities (per feature) to the \n",
    "    #respective probability array -- for ngram (values represent 2-words instead of a word)\n",
    "    for i in range(len(test)):\n",
    "        test_word = test[i]\n",
    "        #very IMPORTANT. Here we are adding an if statement to allow our program to run faster (ignoring words\n",
    "        #that are not present and only looking at words that are). Otherwise, the program would take to long to\n",
    "        #actually be able to run (several hours). Additionally, in the bag of words example, we tried it both ways,\n",
    "        #ignoring 0's and not ignoring 0's. The results/accuracy were essentially identical, so we ignore the 0's\n",
    "        #here to drastically improve run-time with the assumption that accuracy will be similar\n",
    "        #however, we want to make it clear that we understand the non-present words shouldn't just be ignored and \n",
    "        #that we would have done so had it been computationally feasible \n",
    "        if test_word > 0:\n",
    "            count = 0\n",
    "            for row in review_text_train_ngram_np_0:\n",
    "                if row[i] == test_word:\n",
    "                    count += 1\n",
    "            prob0 = count/total_negative\n",
    "            probs_0.append(prob0)\n",
    "\n",
    "            count = 0\n",
    "            for row in review_text_train_ngram_np_1:\n",
    "                if row[i] == test_word:\n",
    "                    count += 1\n",
    "            prob1 = count/total_positive\n",
    "            probs_1.append(prob1)\n",
    "    \n",
    "    #multiply each of the probabilities for the relevant for negative and positive reviews, respectively. \n",
    "    #then multiply the product of these independent probabilities by the probability of being a negative\n",
    "    #review and of being a positive review, respectively \n",
    "    naive_prob_0 = np.prod(probs_0)\n",
    "    naive_prob_1 = np.prod(probs_1)      \n",
    "    \n",
    "    naive_prob_0 = naive_prob_0 * total_negative / (total_negative + total_positive)\n",
    "    naive_prob_1 = naive_prob_1 * total_positive / (total_negative + total_positive)\n",
    "    \n",
    "    #if the probability for getting a negative label given the words is greater than the probability of \n",
    "    #getting a positive label given the words, then classify the review as a negative label\n",
    "    #if it is the other way around, classify it as a positive value. If it is a tie, we arbitrarily classify it\n",
    "    #as a 0\n",
    "    if naive_prob_0 >= naive_prob_1:\n",
    "        test_predicted_labels.append(0)\n",
    "    else:\n",
    "        test_predicted_labels.append(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6090909090909091"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the accuracy with our predicted labels vs. actual labels -- for ngram\n",
    "labels_actual = np.array(testing_labels_all.astype(int)).tolist()\n",
    "accuracy_score(labels_actual, test_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create a confusion matrix for our actual test labels and predicted test labels for the ngram example\n",
    "#uses confusion matrix function defined above to create a visual representation of it \n",
    "plot_confusion_matrix(cm = confusion_matrix(labels_actual, test_predicted_labels), target_names = \n",
    "                      [\"Positive (1)\", \"Negative(0)\"], title = \"Review Sentiment Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6456790899135783"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a logistic regression model with l2 penalty, and print out the mean accuracy score using cross validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l2', tol=0.001, C=5.0, solver='lbfgs', multi_class='multinomial')\n",
    "scores = cross_validate(lr, review_ngram_text_copy, training_labels_all.astype(int), cv=10,scoring=('accuracy'),return_train_score=True)\n",
    "scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the logistic regression model with our text and label training data \n",
    "lr.fit(review_ngram_text_copy, training_labels_all.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary so that each coefficient value has a key that is the word/column it is associated with\n",
    "#sort the dictionary in increasing order\n",
    "coefs = dict()\n",
    "for i in range(len(lr.coef_[0])):\n",
    "    coefs[review_ngram_text_copy.columns[i]] = lr.coef_[0][i]\n",
    "sorted_coefs = sorted(coefs.items(), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('disappoint', ''), -2.4641914330939643),\n",
       " (('wast', 'money'), -2.3294649074775897),\n",
       " (('wast', 'time'), -1.863128666372419),\n",
       " (('dont', 'think'), -1.8182589371052584),\n",
       " (('wont', 'back'), -1.547983579848017),\n",
       " (('dont', 'wast'), -1.53017391272242),\n",
       " (('dont', 'buy'), -1.4301109433579375),\n",
       " (('poor', 'qualiti'), -1.369650653627439),\n",
       " (('avoid', 'cost'), -1.352350760329584),\n",
       " (('stay', 'away'), -1.340295353131793),\n",
       " (('doesnt', 'work'), -1.3296345422494205),\n",
       " (('piec', 'junk'), -1.295118064892778),\n",
       " (('wait', 'wait'), -1.211756276621208),\n",
       " (('food', 'averag'), -1.1410742113884869),\n",
       " (('good', 'way'), -1.130471358106623),\n",
       " (('ever', 'go'), -1.0839543436558283),\n",
       " (('qualiti', 'poor'), -1.0802390740661352),\n",
       " (('look', 'good'), -1.0665957859629625),\n",
       " (('worst', 'ever'), -1.0556680721742169),\n",
       " (('would', 'go'), -1.0393264808690716)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the first 20 key-value pairs in the dictionary\n",
    "#this represents the 20 words that have the strongest sentiment/indication of a negative review\n",
    "sorted_coefs[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('well', 'made'), 1.291864970081487),\n",
       " (('great', 'deal'), 1.297803766512901),\n",
       " (('food', 'good'), 1.3228728076397303),\n",
       " (('pretti', 'good'), 1.3233367396488442),\n",
       " (('good', 'qualiti'), 1.3513813840814028),\n",
       " (('good', 'price'), 1.3586383406129035),\n",
       " (('work', 'fine'), 1.4276539377003943),\n",
       " (('food', 'delici'), 1.42926224877376),\n",
       " (('wont', 'disappoint'), 1.4583908938650416),\n",
       " (('one', 'best'), 1.480364642764635),\n",
       " (('great', 'servic'), 1.4864676018956866),\n",
       " (('5', 'star'), 1.505664855798875),\n",
       " (('work', 'well'), 1.5734620325698845),\n",
       " (('love', 'place'), 1.6146972313211616),\n",
       " (('easi', 'use'), 1.6633690133882837),\n",
       " (('realli', 'good'), 1.7898537560952201),\n",
       " (('great', 'product'), 1.826655061015979),\n",
       " (('highli', 'recommend'), 1.9559973595915314),\n",
       " (('great', 'phone'), 2.3929133323620735),\n",
       " (('work', 'great'), 2.719656931842833)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the last 20 key-value pairs in the dictionary\n",
    "#this represents the 20 words that have the strongest sentiment/indication of a positive review\n",
    "last = len(sorted_coefs)\n",
    "sorted_coefs[last - 20:last]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6479497979860591"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a logistic regression model with l1 penalty, and print out the mean accuracy score using cross validation\n",
    "lr2 = LogisticRegression(penalty='l1', tol=0.01, C = 45.0, solver='saga')\n",
    "scores2 = cross_validate(lr2, review_ngram_text_copy, training_labels_all.astype(int), cv=10,scoring=('accuracy'),return_train_score=True)\n",
    "scores2['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=45.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='saga', tol=0.01, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the logistic regression model with our text and label training data \n",
    "lr2.fit(review_ngram_text_copy, training_labels_all.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary so that each coefficient value has a key that is the word/column it is associated with\n",
    "#sort the dictionary in increasing order\n",
    "coefs2 = dict()\n",
    "for i in range(len(lr2.coef_[0])):\n",
    "    coefs2[review_ngram_text_copy.columns[i]] = lr2.coef_[0][i]\n",
    "sorted_coefs2 = sorted(coefs2.items(), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('wast', 'money'), -2.414566331219471),\n",
       " (('disappoint', ''), -2.0396327952647697),\n",
       " (('dont', 'wast'), -1.9093117824602133),\n",
       " (('wast', 'time'), -1.8673128414004267),\n",
       " (('dont', 'think'), -1.7190480230208236),\n",
       " (('go', 'back'), -1.4611533353514763),\n",
       " (('wont', 'back'), -1.1432793767236513),\n",
       " (('dont', 'buy'), -0.9700134584521419),\n",
       " (('stay', 'away'), -0.9530118686638314),\n",
       " (('doesnt', 'work'), -0.9285317505034545),\n",
       " (('poor', 'qualiti'), -0.927050742578308),\n",
       " (('good', 'way'), -0.8618369143957058),\n",
       " (('ever', 'go'), -0.8571886197951033),\n",
       " (('wait', 'wait'), -0.8327533424596374),\n",
       " (('piec', 'junk'), -0.8325435646560674),\n",
       " (('year', 'old'), -0.7992331981872008),\n",
       " (('bad', 'food'), -0.7853681639090533),\n",
       " (('money', 'time'), -0.7839921570841901),\n",
       " (('never', 'ever'), -0.7824381824884726),\n",
       " (('zero', 'star'), -0.7752428538064574)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the first 20 key-value pairs in the dictionary\n",
    "#this represents the 20 words that have the strongest sentiment/indication of a negative review\n",
    "sorted_coefs2[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('phone', 'work'), 0.9646177226203461),\n",
       " (('love', 'place'), 0.9822668108670249),\n",
       " (('great', 'food'), 1.041128443419636),\n",
       " (('well', 'made'), 1.0640682432686126),\n",
       " (('food', 'great'), 1.0685173212427654),\n",
       " (('good', 'qualiti'), 1.0885925520446),\n",
       " (('food', 'good'), 1.0907201466376006),\n",
       " (('work', 'fine'), 1.1078367140520395),\n",
       " (('food', 'delici'), 1.125243313385499),\n",
       " (('pretti', 'good'), 1.1276106568305904),\n",
       " (('easi', 'use'), 1.1856021996125632),\n",
       " (('5', 'star'), 1.2535509498346051),\n",
       " (('great', 'product'), 1.3988593589663416),\n",
       " (('one', 'best'), 1.418029479195674),\n",
       " (('great', 'servic'), 1.4324051400435456),\n",
       " (('work', 'well'), 1.4473092107629733),\n",
       " (('realli', 'good'), 1.4999664974075808),\n",
       " (('highli', 'recommend'), 1.7882889156164241),\n",
       " (('great', 'phone'), 1.9737156088612244),\n",
       " (('work', 'great'), 3.0814713123851654)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the last 10 key-value pairs in the dictionary\n",
    "#this represents the 10 words that have the strongest sentiment/indication of a positive review\n",
    "last2 = len(sorted_coefs)\n",
    "sorted_coefs2[last - 20:last]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
